{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为这个项目导入需要的库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from IPython.display import display # 允许为DataFrame使用display()\n",
    "\n",
    "# 导入附加的可视化代码visuals.py\n",
    "import visuals as vs\n",
    "\n",
    "# 魔法函数，功能是可以内嵌绘图，并且可以省略掉plt.show()这一步\n",
    "%matplotlib inline\n",
    "\n",
    "# 导入人口普查数据\n",
    "data = pd.read_csv(\"census.csv\")\n",
    "\n",
    "# 成功 - 显示第一条记录\n",
    "display(data.head(n=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_records = data.shape[0]\n",
    "\n",
    "from pandas import Series\n",
    "n_greater_50k = pd.value_counts(data.income == \">50K\")[True]\n",
    "n_at_most_50k = pd.value_counts(data.income == \"<=50K\")[True]\n",
    "\n",
    "greater_percent = n_greater_50k/n_records*100\n",
    "\n",
    "# 打印结果\n",
    "print (\"Total number of records: {}\".format(n_records))\n",
    "print (\"Individuals making more than $50,000: {}\".format(n_greater_50k))\n",
    "print (\"Individuals making at most $50,000: {}\".format(n_at_most_50k))\n",
    "print (\"Percentage of individuals making more than $50,000: {:.2f}%\".format(greater_percent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将数据切分成特征和对应的标签\n",
    "income_raw = data['income']\n",
    "features_raw = data.drop('income', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化 'capital-gain'和'capital-loss' 两个特征\n",
    "vs.distribution(features_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对于倾斜的数据使用Log转换\n",
    "skewed = ['capital-gain', 'capital-loss']\n",
    "features_raw[skewed] = data[skewed].apply(lambda x: np.log(x + 1))\n",
    "\n",
    "# 可视化对数转换后 'capital-gain'和'capital-loss' 两个特征\n",
    "vs.distribution(features_raw, transformed = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# 初始化一个 scaler，并将它施加到特征上\n",
    "scaler = MinMaxScaler()\n",
    "numerical = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "features_raw[numerical] = scaler.fit_transform(data[numerical])\n",
    "\n",
    "# 显示一个经过缩放的样例记录\n",
    "display(features_raw.head(n = 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.get_dummies(features_raw)\n",
    "income = income_raw.map({'<=50K':0, '>50K':1})\n",
    "\n",
    "# 打印经过独热编码之后的特征数量\n",
    "encoded = list(features.columns)\n",
    "print (\"{} total features after one-hot encoding.\".format(len(encoded)))\n",
    "print(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 将'features'和'income'数据切分成训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, income, test_size = 0.2, random_state = 0,\n",
    "                                                    stratify = income)\n",
    "# 将'X_train'和'y_train'进一步切分为训练集和验证集\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0,\n",
    "                                                    stratify = y_train)\n",
    "\n",
    "# 显示切分的结果\n",
    "print (\"Training set has {} samples.\".format(X_train.shape[0]))\n",
    "print (\"Validation set has {} samples.\".format(X_val.shape[0]))\n",
    "print (\"Testing set has {} samples.\".format(X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = n_greater_50k/n_records\n",
    "precision = n_greater_50k/n_records\n",
    "recall = n_greater_50k/n_greater_50k\n",
    "\n",
    "beta = 0.5\n",
    "fscore = (1 + pow(beta, 2))*(precision*recall)/((pow(beta, 2)*precision) + recall)\n",
    "\n",
    "# 打印结果\n",
    "print (\"Naive Predictor on validation data: \\n \\\n",
    "    Accuracy score: {:.4f} \\n \\\n",
    "    Precision: {:.4f} \\n \\\n",
    "    Recall: {:.4f} \\n \\\n",
    "    F-score: {:.4f}\".format(accuracy, precision, recall, fscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, accuracy_score\n",
    "\n",
    "def train_predict(learner, sample_size, X_train, y_train, X_val, y_val): \n",
    "    '''\n",
    "    inputs:\n",
    "       - learner: the learning algorithm to be trained and predicted on\n",
    "       - sample_size: the size of samples (number) to be drawn from training set\n",
    "       - X_train: features training set\n",
    "       - y_train: income training set\n",
    "       - X_val: features validation set\n",
    "       - y_val: income validation set\n",
    "    '''\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    start = time() # 获得程序开始时间\n",
    "    learner = learner.fit(X_train[:sample_size], y_train[:sample_size])\n",
    "    end = time() # 获得程序结束时间\n",
    "    \n",
    "    results['train_time'] = end - start\n",
    "    \n",
    "    # 然后得到对前300个训练数据的预测结果\n",
    "    start = time() # 获得程序开始时间\n",
    "    predictions_val = learner.predict(X_val)\n",
    "    predictions_train = learner.predict(X_train[:300])\n",
    "    end = time() # 获得程序结束时间\n",
    "    \n",
    "    # 计算预测用时\n",
    "    results['pred_time'] = end - start\n",
    "            \n",
    "    # 计算在最前面的300个训练数据的准确率\n",
    "    results['acc_train'] = accuracy_score(y_train[:300], predictions_train)\n",
    "        \n",
    "    # 计算在验证上的准确率\n",
    "    results['acc_val'] = accuracy_score(y_val,predictions_val)\n",
    "    \n",
    "    # 计算在最前面300个训练数据上的F-score\n",
    "    results['f_train'] = fbeta_score(y_train[: 300], predictions_train, beta=0.5)\n",
    "        \n",
    "    # 计算验证集上的F-score\n",
    "    results['f_val'] = fbeta_score(y_val, predictions_val, beta=0.5)\n",
    "       \n",
    "    print (\"{} trained on {} samples.\".format(learner.__class__.__name__, sample_size))\n",
    "        \n",
    "    # 返回结果\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从sklearn中导入三个监督学习模型\n",
    "from sklearn import tree, svm, ensemble\n",
    "\n",
    "# 初始化三个模型\n",
    "clf_A = tree.DecisionTreeClassifier(random_state = 0)\n",
    "clf_B = svm.SVC(random_state = 0)\n",
    "clf_C = ensemble.AdaBoostClassifier(random_state = 0)\n",
    "\n",
    "# 计算1%， 10%， 100%的训练数据分别对应多少点\n",
    "samples_1 = int(X_train.shape[0]*0.01)\n",
    "samples_10 = int(X_train.shape[0]*0.1)\n",
    "samples_100 = int(X_train.shape[0] * 1)\n",
    "print ([samples_1, samples_10, samples_100])\n",
    "\n",
    "# 收集学习器的结果\n",
    "results = {}\n",
    "for clf in [clf_A, clf_B, clf_C]:\n",
    "    clf_name = clf.__class__.__name__\n",
    "    results[clf_name] = {}\n",
    "    for i, samples in enumerate([samples_1, samples_10, samples_100]):\n",
    "        results[clf_name][i] = train_predict(clf, samples, X_train, y_train, X_val, y_val)\n",
    "\n",
    "# 对选择的三个模型得到的评价结果进行可视化\n",
    "vs.evaluate(results, accuracy, fscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入'GridSearchCV', 'make_scorer'和其他一些需要的库\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# 初始化分类器\n",
    "clf = AdaBoostClassifier(random_state=0)\n",
    "\n",
    "# 创建你希望调节的参数列表\n",
    "parameters = {'n_estimators': [50, 100, 200, 300]}\n",
    "\n",
    "# 创建一个fbeta_score打分对象\n",
    "scorer = make_scorer(fbeta_score, beta=0.5)\n",
    "\n",
    "# 在分类器上使用网格搜索，使用'scorer'作为评价函数\n",
    "kfold = KFold(n_splits=10)\n",
    "grid_obj = GridSearchCV(clf, parameters, scorer, cv=kfold)\n",
    "\n",
    "# 用训练数据拟合网格搜索对象并找到最佳参数\n",
    "grid_fit = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# 得到estimator\n",
    "best_clf = grid_obj.best_estimator_\n",
    "\n",
    "# 使用没有调优的模型做预测\n",
    "predictions = (clf.fit(X_train, y_train)).predict(X_val)\n",
    "best_predictions = best_clf.predict(X_val)\n",
    "\n",
    "# 汇报调优后的模型\n",
    "print (\"best_clf\\n------\")\n",
    "print (best_clf)\n",
    "\n",
    "# 汇报调参前和调参后的分数\n",
    "print (\"\\nUnoptimized model\\n------\")\n",
    "print (\"Accuracy score on validation data: {:.4f}\".format(accuracy_score(y_val, predictions)))\n",
    "print (\"F-score on validation data: {:.4f}\".format(fbeta_score(y_val, predictions, beta = 0.5)))\n",
    "print (\"\\nOptimized Model\\n------\")\n",
    "print (\"Final accuracy score on the validation data: {:.4f}\".format(accuracy_score(y_val, best_predictions)))\n",
    "print (\"Final F-score on the validation data: {:.4f}\".format(fbeta_score(y_val, best_predictions, beta = 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入一个有'feature_importances_'的监督学习模型\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 在训练集上训练一个监督学习模型\n",
    "model = RandomForestClassifier(random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 提取特征重要性\n",
    "importances = model.feature_importances_\n",
    "#importances_AdaBoost = best_clf.feature_importances_\n",
    "\n",
    "# 绘图\n",
    "vs.feature_plot(importances, X_train, y_train)\n",
    "#vs.feature_plot(importances_AdaBoost, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入克隆模型的功能\n",
    "from sklearn.base import clone\n",
    "\n",
    "# 减小特征空间\n",
    "X_train_reduced = X_train[X_train.columns.values[(np.argsort(importances)[::-1])[:5]]]\n",
    "X_val_reduced = X_val[X_val.columns.values[(np.argsort(importances)[::-1])[:5]]]\n",
    "\n",
    "# 在前面的网格搜索的基础上训练一个“最好的”模型\n",
    "clf_on_reduced = (clone(best_clf)).fit(X_train_reduced, y_train)\n",
    "\n",
    "# 做一个新的预测\n",
    "reduced_predictions = clf_on_reduced.predict(X_val_reduced)\n",
    "\n",
    "# 对于每一个版本的数据汇报最终模型的分数\n",
    "print (\"Final Model trained on full data\\n------\")\n",
    "print (\"Accuracy on validation data: {:.4f}\".format(accuracy_score(y_val, best_predictions)))\n",
    "print (\"F-score on validation data: {:.4f}\".format(fbeta_score(y_val, best_predictions, beta = 0.5)))\n",
    "print (\"\\nFinal Model trained on reduced data\\n------\")\n",
    "print (\"Accuracy on validation data: {:.4f}\".format(accuracy_score(y_val, reduced_predictions)))\n",
    "print (\"F-score on validation data: {:.4f}\".format(fbeta_score(y_val, reduced_predictions, beta = 0.5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_test = best_clf.predict(X_test)\n",
    "print (\"Accuracy on best_clf is {:.4f}\" .format(accuracy_score(y_test, predictions_test)))\n",
    "print (\"F-Score on best_clf is {:.4f}\" .format(fbeta_score(y_test, predictions_test, beta=0.5)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
